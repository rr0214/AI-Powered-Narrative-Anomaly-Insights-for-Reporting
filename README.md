# ğŸš€ AI-Powered Narrative & Anomaly Insights for Reporting

*Demonstrating how AI can streamline financial/ESG reporting by automatically generating executive-ready narratives and flagging anomalies from structured data.*

## ğŸ“– Project Overview

**Goal:** Demonstrate how AI can streamline financial/ESG reporting by automatically generating executive-ready narratives and flagging anomalies from structured data.

**Why Workiva:** Aligns with Workiva's mission of connected, compliant reporting by showing how LLMs + lightweight stats can save hours of analyst effort, reduce errors, and increase transparency.

---

## ğŸ“ˆ Go-To-Market (GTM) Narrative

### Target Users
- Compliance officers
- Financial controllers  
- ESG managers

### Value Proposition
- **Automate** first-draft narratives and anomaly explanations
- **Save** 5â€“10 hours per filing cycle
- **Reduce risk** of oversight with structured anomaly flagging

### Differentiation
- **Embedded** directly in Workiva workflows (no copy-paste to/from external tools)
- **Built-in** audit trail + governance controls (critical for enterprise)

### Launch Motion
1. Start with a "Smart Summary" beta in SEC reporting workflows
2. Collect adoption metrics (time saved, % of reports using AI draft)
3. Expand to ESG, Risk, and Audit modules

---

## ğŸ“ Product Requirements Document (PRD) Snapshot

### Problem Statement
Analysts spend too much time drafting repetitive narratives and scanning for anomalies.

### Proposed Solution
AI-generated summaries and anomaly callouts from connected data sources, with human review.

### Success Metrics
- **30%** reduction in report prep time
- **90%** accuracy of metrics cited in AI output
- **â‰¥70%** adoption in pilot teams

---

## ğŸ‘©â€ğŸ’» User Stories & Acceptance Criteria

### User Story 1: AI-Generated Narratives
**As a compliance manager, I want AI-generated narrative summaries of quarterly metrics so I can focus on review instead of drafting.**

**Acceptance Criteria:**
- âœ… Upload/ingest data file
- âœ… Receive draft narrative (3â€“5 sentences)
- âœ… Numbers must match source metrics

### User Story 2: Anomaly Detection & Explanation
**As a financial analyst, I want anomalies flagged and explained so I don't miss critical risks.**

**Acceptance Criteria:**
- âœ… Outlier detection (z-score â‰¥2 or IQR bounds)
- âœ… LLM explains business context in <80 words
- âœ… Each anomaly includes suggested follow-up

### User Story 3: Audit Trail & Compliance
**As an auditor, I want an audit trail of AI vs human edits so compliance is maintained.**

**Acceptance Criteria:**
- âœ… All AI outputs logged
- âœ… Provenance shows source rows/columns

---

## ğŸ—ï¸ Architecture Diagram

```
            +-------------+
Upload CSV â†’ |   pandas    | â†’ stats (QoQ, z-score, IQR)
            +-------------+
                    |
            +-------------------+
            |   PII Scrubber    |
            +-------------------+
                    |
            +----------------------------+
            |   LLM (OpenAI GPT-4o-mini) |
            +----------------------------+
                    |
            +-------------------+
            |   Validator (pydantic)  |
            +-------------------+
                    |
            +-------------------+
            | Streamlit UI (view, edit, export DOCX) |
            +-------------------+
```

---

## ğŸ›£ï¸ Roadmap: From Orchestration to Agentic AI

### Phase 1: MVP (Today)
- Upload â†’ AI narrative + anomaly callouts â†’ export
- Human review loop in Streamlit

### Phase 2: Enterprise Orchestration
- Integrate with Snowflake/Sheets APIs
- n8n automations: nightly jobs â†’ Slack notifications â†’ auto-doc export
- API hooks for multi-team workflows

### Phase 3: Agentic AI Workflows
**AI agent autonomously:**
- Monitors incoming data streams
- Detects anomalies
- Drafts narratives
- Routes draft via approval chain (Slack/Jira)
- Submits final doc into Workiva filing pipeline

**Guardrails:** Human-in-the-loop approvals, full audit logs, provenance tags

### Phase 4: Full Platform Integration
- Extend to ESG, Risk, Audit modules
- Multi-agent collaboration (e.g., "Narrative Agent" + "Risk Agent" + "Compliance Agent")
- MCP-enabled tool layer for secure, permissioned access to data + exports

---

## âœ… Output Evaluation Criteria

To ensure AI-generated narratives and anomaly explanations are reliable, outputs are evaluated against the following dimensions:

### 1. Accuracy
- **Numerical integrity:** All numbers in the AI output must exactly match source data
- **Consistency:** No contradictory statements across different sections of the report

### 2. Relevance
- **On-topic:** Summaries must reference only provided metrics (no hallucinated KPIs)
- **Anomaly coverage:** All flagged anomalies (z-score â‰¥2, IQR outliers) must be addressed in the narrative

### 3. Clarity
- **Readability:** Plain business English, â‰¤12th grade reading level
- **Conciseness:** Summaries â‰¤5 sentences, anomaly explanations â‰¤80 words
- **Executive tone:** Neutral, fact-based, avoids speculation unless marked as "possible causes"

### 4. Auditability
- **Provenance:** Each figure or anomaly explanation cites the exact source column/row
- **Versioning:** All AI outputs logged with timestamp + prompt + response
- **Traceability:** Audit trail shows human edits vs AI draft

### 5. Robustness
- **Boundary handling:** Borderline z-scores (2.0â€“2.2) labeled "borderline" rather than "critical"
- **Missing data:** Graceful degradation (skip or annotate missing values rather than invent)
- **Bias check:** Ensure language is neutral (no loaded terms like "failure," "catastrophic")

### 6. Adoption Metrics
- **Human acceptance rate:** % of AI sentences kept vs edited
- **Analyst time saved:** Minutes saved per filing cycle
- **Coverage:** % of filings/reports using AI draft

---

## ğŸ“Š Metrics & Next Steps

### Pilot KPIs
- Time saved
- Adoption rate
- Accuracy of metrics

### Expansion KPIs
- Filing error reduction
- % anomalies explained
- Cross-module adoption

### Future Work
- MCP integration for secure tool access
- Local-model option (for data residency)
- Continuous evaluation dashboard

---

