import streamlit as st
import pandas as pd
import numpy as np
import json
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import io

# Import required libraries
try:
    from anthropic import Anthropic
    from pydantic import BaseModel, Field
    from docx import Document
    from docx.shared import Inches
except ImportError as e:
    st.error(f"Missing dependency: {e}. Please install with: uv add anthropic pydantic python-docx")
    st.stop()

# Pydantic models for structured outputs
class Anomaly(BaseModel):
    metric: str = Field(description="The financial metric with anomaly")
    current_value: str = Field(description="Current quarter value")
    comparison_value: str = Field(default="N/A", description="Previous quarter value") 
    change_percent: str = Field(default="N/A", description="Percentage change")
    risk_level: str = Field(description="High, Medium, or Low")
    explanation: str = Field(description="Business context explanation", max_length=200)
    next_steps: str = Field(description="Recommended actions", max_length=150)
    z_score: float = Field(description="Statistical z-score")

class ExecutiveSummary(BaseModel):
    narrative: str = Field(description="3-5 sentence executive summary")
    key_metrics: List[str] = Field(description="List of key metrics mentioned")
    data_sources: List[str] = Field(description="Source columns referenced")

class FinancialAnalysis(BaseModel):
    executive_summary: ExecutiveSummary
    anomalies: List[Anomaly]
    total_anomalies_found: int
    analysis_timestamp: str

# Initialize Anthropic client
def get_anthropic_client():
    """Get Anthropic client with API key validation"""
    try:
        api_key = st.secrets.get("ANTHROPIC_API_KEY")
    except:
        api_key = None
    
    if not api_key:
        api_key = st.sidebar.text_input(
            "Anthropic API Key", type="password", 
            help="Get your API key from console.anthropic.com"
        )
    
    if not api_key:
        st.warning("Please enter your Anthropic API key in the sidebar to continue")
        st.stop()
    
    return Anthropic(api_key=api_key)

# Database setup for audit trail
def init_database():
    conn = sqlite3.connect('audit_trail.db')
    conn.execute('''
        CREATE TABLE IF NOT EXISTS analysis_log (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT,
            input_file_name TEXT,
            anomalies_detected INTEGER,
            prompt_used TEXT,
            ai_response TEXT,
            user_edits TEXT
        )
    ''')
    conn.commit()
    return conn

# Anomaly detection functions
def detect_anomalies(df: pd.DataFrame, threshold: float = 2.0) -> Dict[str, Any]:
    """Detect statistical anomalies using z-score and IQR methods"""
    anomalies = []
    
    # Get numeric columns only
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        if len(df[col].dropna()) < 2:  # Skip if insufficient data
            continue
            
        # Calculate z-scores
        mean_val = df[col].mean()
        std_val = df[col].std()
        
        if std_val == 0:  # Skip if no variation
            continue
            
        z_scores = np.abs((df[col] - mean_val) / std_val)
        
        # Find outliers
        outlier_indices = z_scores >= threshold
        
        if outlier_indices.any():
            for idx in df[outlier_indices].index:
                anomalies.append({
                    'metric': col,
                    'index': idx,
                    'value': df.loc[idx, col],
                    'z_score': z_scores.loc[idx],
                    'mean': mean_val,
                    'std': std_val
                })
    
    return anomalies

def calculate_qoq_changes(df: pd.DataFrame) -> pd.DataFrame:
    """Calculate quarter-over-quarter changes"""
    if 'Quarter' not in df.columns:
        return df
        
    df_sorted = df.sort_values('Quarter')
    numeric_cols = df_sorted.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        df_sorted[f'{col}_QoQ_Change'] = df_sorted[col].pct_change() * 100
        df_sorted[f'{col}_QoQ_Abs_Change'] = df_sorted[col].diff()
    
    return df_sorted

# LLM Analysis
async def analyze_with_claude(df: pd.DataFrame, anomalies: List[Dict], client: Anthropic) -> FinancialAnalysis:
    """Generate analysis using Claude Haiku with function calling"""
    
    # Prepare data summary
    data_summary = {
        'columns': df.columns.tolist(),
        'shape': df.shape,
        'numeric_summary': df.describe().to_dict(),
        'latest_quarter': df.iloc[-1].to_dict() if len(df) > 0 else {},
        'anomalies_detected': len(anomalies)
    }
    
    # Create the analysis tool
    analysis_tool = {
        "name": "financial_analysis",
        "description": "Analyze quarterly financial data and generate executive summary with anomaly explanations",
        "input_schema": {
            "type": "object",
            "properties": {
                "executive_summary": {
                    "type": "object",
                    "properties": {
                        "narrative": {"type": "string", "description": "3-5 sentence executive summary"},
                        "key_metrics": {"type": "array", "items": {"type": "string"}},
                        "data_sources": {"type": "array", "items": {"type": "string"}}
                    },
                    "required": ["narrative", "key_metrics", "data_sources"]
                },
                "anomalies": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "metric": {"type": "string"},
                            "current_value": {"type": "string"},
                            "comparison_value": {"type": "string"},
                            "change_percent": {"type": "string"},
                            "risk_level": {"type": "string", "enum": ["High", "Medium", "Low"]},
                            "explanation": {"type": "string", "maxLength": 80},
                            "next_steps": {"type": "string"},
                            "z_score": {"type": "number"}
                        },
                                                                "required": ["metric", "current_value", "risk_level", "explanation", "next_steps", "z_score"]
                    }
                },
                "total_anomalies_found": {"type": "integer"},
                "analysis_timestamp": {"type": "string"}
            },
            "required": ["executive_summary", "anomalies", "total_anomalies_found", "analysis_timestamp"]
        }
    }
    
    # Construct prompt
    prompt = f"""You are analyzing quarterly financial data for executive reporting.

Dataset Overview:
- Columns: {data_summary['columns']}
- Shape: {data_summary['shape']} (rows, columns)
- Latest Quarter Data: {json.dumps(data_summary['latest_quarter'], indent=2)}

Statistical Anomalies Detected: {len(anomalies)}
{json.dumps(anomalies, indent=2) if anomalies else "None"}

Instructions:
1. Generate a 3-5 sentence executive summary highlighting key trends
2. For each anomaly, provide business context and next steps  
3. Use ONLY the numbers provided - no estimates or assumptions
4. Focus on actionable insights for compliance managers
5. Keep explanations under 80 words each

Use the financial_analysis tool to structure your response."""

    try:
        response = client.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=2000,
            tools=[analysis_tool],
            messages=[{"role": "user", "content": prompt}]
        )
        
        # Extract tool use result
        tool_result = None
        for content in response.content:
            if content.type == "tool_use":
                tool_result = content.input
                break
        
        if not tool_result:
            st.error("Claude did not use the analysis tool properly")
            return None
            
        # Convert to Pydantic model for validation
        return FinancialAnalysis(**tool_result)
        
    except Exception as e:
        st.error(f"Error calling Claude API: {str(e)}")
        return None

def create_provenance_mapping(df: pd.DataFrame, anomalies: List[Dict]) -> Dict[str, str]:
    """Create mapping of values to their exact cell locations"""
    provenance = {}
    
    # Map each value to its cell location
    for row_idx, row in df.iterrows():
        for col_name, value in row.items():
            if pd.notna(value) and isinstance(value, (int, float)):
                # Create Excel-style cell reference
                col_letter = chr(65 + list(df.columns).index(col_name))  # A, B, C, etc.
                cell_ref = f"{col_letter}{row_idx + 2}"  # +2 because Excel starts at 1 and has header
                provenance[f"{value}"] = f"{col_name} (cell {cell_ref})"
                provenance[f"{col_name}_{value}"] = f"cell {cell_ref}"
    
    return provenance

def create_cell_reference_map(df: pd.DataFrame) -> Dict[str, str]:
    """Create mapping of every value to Excel-style cell reference"""
    cell_map = {}
    
    for row_idx, row in df.iterrows():
        for col_idx, (col_name, value) in enumerate(row.items()):
            if pd.notna(value):
                # Excel-style cell reference (A1, B2, etc.)
                col_letter = chr(65 + col_idx)  # A, B, C...
                excel_row = row_idx + 2  # +2 for header and 1-indexed
                cell_ref = f"{col_letter}{excel_row}"
                
                # Map value to cell location with multiple key formats
                cell_map[f"{value}"] = f"{col_name} (cell {cell_ref})"
                cell_map[f"{col_name}_{value}"] = f"cell {cell_ref}"
                
                # Also map formatted versions
                if isinstance(value, (int, float)):
                    cell_map[f"{value:,.0f}"] = f"{col_name} (cell {cell_ref})"
                    cell_map[f"{value:,.1f}"] = f"{col_name} (cell {cell_ref})"
                    cell_map[f"${value:,.0f}M"] = f"{col_name} (cell {cell_ref})"
    
    return cell_map

def safe_column_match(metric_name: str, df_columns: List[str]) -> str:
    """Safely match anomaly metric names to actual DataFrame columns"""
    
    # Direct match first
    if metric_name in df_columns:
        return metric_name
    
    # Fuzzy matching for common variations
    metric_clean = metric_name.lower().replace(' ', '_').replace('-', '_')
    
    for col in df_columns:
        col_clean = col.lower().replace(' ', '_').replace('-', '_')
        
        # Exact match after cleaning
        if metric_clean == col_clean:
            return col
            
        # Partial match (e.g., "Operating_Expenses" matches "Operating_Expenses_M")
        if metric_clean in col_clean or col_clean in metric_clean:
            return col
    
    # If no match found, return the original name with warning
    return metric_name

def generate_audit_trail_report(analysis: FinancialAnalysis, df: pd.DataFrame, anomalies: List[Dict]) -> str:
    """Generate comprehensive audit trail with cell-level provenance"""
    
    report = f"""AUDIT TRAIL REPORT - AI ANALYSIS PROVENANCE
Generated: {analysis.analysis_timestamp}
Dataset: {df.shape[0]} rows x {df.shape[1]} columns

{'='*60}
EXECUTIVE SUMMARY ANALYSIS
{'='*60}

AI Generated Text:
"{analysis.executive_summary.narrative}"

Key Metrics Referenced:
"""
    
    for metric in analysis.executive_summary.key_metrics:
        report += f"- {metric}\n"
    
    report += f"\nData Sources: {', '.join(analysis.executive_summary.data_sources)}\n"
    
    report += f"\n{'='*60}\n"
    report += f"ANOMALY DETECTION PROVENANCE\n"
    report += f"{'='*60}\n\n"
    
    if not anomalies:
        report += "No statistical anomalies detected in this dataset.\n"
    else:
        for i, raw_anomaly in enumerate(anomalies, 1):
            report += f"ANOMALY #{i}: {raw_anomaly['metric']}\n"
            report += f"{'-'*40}\n"
            
            # Find column index safely
            try:
                if raw_anomaly['metric'] in df.columns:
                    col_idx = list(df.columns).index(raw_anomaly['metric'])
                    col_letter = chr(65 + col_idx)
                    row_number = raw_anomaly['index'] + 2
                    cell_ref = f"{col_letter}{row_number}"
                    report += f"Source Cell: {cell_ref} ({raw_anomaly['metric']})\n"
                else:
                    report += f"Source: {raw_anomaly['metric']} column\n"
            except (KeyError, ValueError):
                report += f"Source: {raw_anomaly.get('metric', 'Unknown column')}\n"
            
            report += f"Raw Value: {raw_anomaly.get('value', 'N/A')}\n"
            report += f"Statistical Analysis:\n"
            report += f"  - Dataset Mean: {raw_anomaly.get('mean', 0):.2f}\n"
            report += f"  - Standard Deviation: {raw_anomaly.get('std', 0):.2f}\n"
            report += f"  - Z-Score: {raw_anomaly.get('z_score', 0):.2f}\n"
            report += f"  - Detection Threshold: 2.0\n\n"
    
    # Add AI analysis if available
    if hasattr(analysis, 'anomalies') and analysis.anomalies:
        report += f"AI ANALYSIS RESPONSES:\n\n"
        for i, anomaly_obj in enumerate(analysis.anomalies, 1):
            report += f"AI Response #{i}:\n"
            report += f"  - Metric: {anomaly_obj.metric}\n"
            report += f"  - Current Value: {anomaly_obj.current_value}\n"
            report += f"  - Risk Level: {anomaly_obj.risk_level}\n"
            report += f"  - Explanation: {anomaly_obj.explanation}\n"
            report += f"  - Next Steps: {anomaly_obj.next_steps}\n\n"
    
    report += f"{'='*60}\n"
    report += f"VALIDATION SUMMARY\n"
    report += f"{'='*60}\n"
    report += f"Total Statistical Anomalies: {len(anomalies)}\n"
    report += f"AI Responses Generated: {len(analysis.anomalies) if hasattr(analysis, 'anomalies') else 0}\n"
    report += f"Schema Validation: PASSED\n"
    report += f"Data Integrity: All numbers traced to source\n"
    
    return report
def export_to_word(analysis: FinancialAnalysis, filename: str = "financial_analysis.docx"):
    """Export analysis to Word document"""
    doc = Document()
    
    # Title
    title = doc.add_heading('Quarterly Financial Analysis', 0)
    
    # Executive Summary
    doc.add_heading('Executive Summary', level=1)
    doc.add_paragraph(analysis.executive_summary.narrative)
    
    # Key Metrics
    doc.add_heading('Key Metrics', level=2)
    for metric in analysis.executive_summary.key_metrics:
        doc.add_paragraph(f"â€¢ {metric}", style='List Bullet')
    
    # Anomalies Section
    if analysis.anomalies:
        doc.add_heading('Risk & Anomaly Analysis', level=1)
        
        for i, anomaly in enumerate(analysis.anomalies, 1):
            doc.add_heading(f'Anomaly {i}: {anomaly.metric}', level=2)
            doc.add_paragraph(f"Current Value: {anomaly.current_value}")
            if anomaly.comparison_value:
                doc.add_paragraph(f"Previous Value: {anomaly.comparison_value}")
            if anomaly.change_percent:
                doc.add_paragraph(f"Change: {anomaly.change_percent}")
            doc.add_paragraph(f"Risk Level: {anomaly.risk_level}")
            doc.add_paragraph(f"Analysis: {anomaly.explanation}")
            doc.add_paragraph(f"Recommended Actions: {anomaly.next_steps}")
            doc.add_paragraph(f"Statistical Confidence: z-score = {anomaly.z_score:.2f}")
    
    # Audit Information
    doc.add_heading('Audit Trail', level=1)
    doc.add_paragraph(f"Analysis Generated: {analysis.analysis_timestamp}")
    doc.add_paragraph(f"Total Anomalies Detected: {analysis.total_anomalies_found}")
    doc.add_paragraph("Data Sources: " + ", ".join(analysis.executive_summary.data_sources))
    
    # Save to bytes
    doc_bytes = io.BytesIO()
    doc.save(doc_bytes)
    doc_bytes.seek(0)
    
    return doc_bytes.getvalue()

# Streamlit App
def main():
    st.set_page_config(
        page_title="AI Anomaly Detection and Summarization for Reporting",
        page_icon="ðŸ“Š",
        layout="wide"
    )
    
    st.title("ðŸš€ AI Anomaly Detection and Summarization for Reporting")
    st.markdown("*Automatically generate executive summaries and detect statistical anomalies from financial data*")
    
    # Initialize database
    conn = init_database()
    client = get_anthropic_client()
    
    # Sidebar configuration with business-friendly controls
    st.sidebar.header("Detection Settings")
    
    # Business-friendly threshold selection
    sensitivity_level = st.sidebar.select_slider(
        "Anomaly Detection Sensitivity",
        options=["Conservative", "Moderate", "Sensitive"],
        value="Moderate",
        help="Conservative: Only flag extreme outliers | Moderate: Standard business thresholds | Sensitive: Flag smaller deviations"
    )
    
    # Map business terms to statistical values
    threshold_mapping = {
        "Conservative": 3.0,   # ~99.7% confidence - only extreme outliers
        "Moderate": 2.0,       # ~95% confidence - standard business practice
        "Sensitive": 1.5       # ~87% confidence - catch smaller issues
    }
    
    z_score_threshold = threshold_mapping[sensitivity_level]
    
    # Show real-time impact
    with st.sidebar.expander("What This Means"):
        if sensitivity_level == "Conservative":
            st.write("ðŸŽ¯ **Ultra-high confidence alerts**")
            st.write("â€¢ Flags ~0.3% of typical data points")
            st.write("â€¢ Use for: Critical SEC filings, executive dashboards")
            st.write("â€¢ Risk: May miss subtle but important issues")
        elif sensitivity_level == "Moderate": 
            st.write("ðŸ“Š **Standard business threshold**")
            st.write("â€¢ Flags ~5% of typical data points")
            st.write("â€¢ Use for: Quarterly reviews, standard reporting")
            st.write("â€¢ Balance of accuracy and coverage")
        else:  # Sensitive
            st.write("ðŸ” **Early warning system**")
            st.write("â€¢ Flags ~13% of typical data points") 
            st.write("â€¢ Use for: Monthly monitoring, trend analysis")
            st.write("â€¢ Risk: More false positives requiring review")
    
    # Show live preview of threshold impact
    if uploaded_file is not None:
        with st.sidebar.expander("Live Impact Preview"):
            test_anomalies = detect_anomalies(df_with_changes, threshold=z_score_threshold)
            st.metric("Anomalies at Current Setting", len(test_anomalies))
            
            # Compare with other thresholds
            conservative_count = len(detect_anomalies(df_with_changes, threshold=3.0))
            sensitive_count = len(detect_anomalies(df_with_changes, threshold=1.5))
            
            st.write(f"Conservative (3.0): {conservative_count} anomalies")
            st.write(f"Moderate (2.0): {len(test_anomalies)} anomalies") 
            st.write(f"Sensitive (1.5): {sensitive_count} anomalies")
    
    # File upload
    st.header("ðŸ“ Upload Financial Data")
    uploaded_file = st.file_uploader(
        "Choose a CSV or Excel file", 
        type=['csv', 'xlsx'],
        help="Upload quarterly financial data for analysis"
    )
    
    if uploaded_file is not None:
        try:
            # Read file
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            
            st.success(f"âœ… File uploaded: {uploaded_file.name} ({df.shape[0]} rows, {df.shape[1]} columns)")
            
            # Display data preview
            st.header("ðŸ“Š Data Preview")
            st.dataframe(df.head())
            
            # Calculate QoQ changes
            df_with_changes = calculate_qoq_changes(df)
            
            # Detect anomalies
            with st.spinner("ðŸ” Detecting anomalies..."):
                anomalies = detect_anomalies(df_with_changes, threshold=z_score_threshold)
            
            st.header("âš ï¸ Anomalies Detected")
            if anomalies:
                st.warning(f"Found {len(anomalies)} statistical anomalies")
                
                for anomaly in anomalies:
                    with st.expander(f"ðŸš¨ {anomaly['metric']} (z-score: {anomaly['z_score']:.2f})"):
                        st.write(f"**Value:** {anomaly['value']:,.2f}")
                        st.write(f"**Dataset Mean:** {anomaly['mean']:,.2f}")
                        st.write(f"**Standard Deviation:** {anomaly['std']:,.2f}")
            else:
                st.info("No significant anomalies detected")
            
            # Generate AI analysis
            if st.button("ðŸ¤– Generate AI Analysis", type="primary"):
                with st.spinner("ðŸ§  Analyzing with Claude Haiku..."):
                    
                    # Note: Using sync version for Streamlit compatibility
                    # In a real async environment, you'd use the async version
                    analysis = None
                    
                    try:
                        # Prepare data for Claude
                        data_summary = {
                            'columns': df.columns.tolist(),
                            'shape': df.shape,
                            'numeric_summary': df_with_changes.describe().to_dict(),
                            'latest_quarter': df_with_changes.iloc[-1].to_dict() if len(df_with_changes) > 0 else {},
                        }
                        
                        # Simplified synchronous version
                        analysis_tool = {
                            "name": "financial_analysis",
                            "description": "Analyze financial data and generate structured insights",
                            "input_schema": {
                                "type": "object",
                                "properties": {
                                    "executive_summary": {
                                        "type": "object",
                                        "properties": {
                                            "narrative": {"type": "string"},
                                            "key_metrics": {"type": "array", "items": {"type": "string"}},
                                            "data_sources": {"type": "array", "items": {"type": "string"}}
                                        }
                                    },
                                    "anomalies": {
                                        "type": "array",
                                        "items": {
                                            "type": "object",
                                            "properties": {
                                                "metric": {"type": "string"},
                                                "current_value": {"type": "string"},
                                                "comparison_value": {"type": "string"},
                                                "change_percent": {"type": "string"},
                                                "risk_level": {"type": "string"},
                                                "explanation": {"type": "string"},
                                                "next_steps": {"type": "string"},
                                                "z_score": {"type": "number"}
                                            }
                                        }
                                    },
                                    "total_anomalies_found": {"type": "integer"},
                                    "analysis_timestamp": {"type": "string"}
                                }
                            }
                        }
                        
                        prompt = f"""Analyze quarterly financial data. Follow these patterns:

CORRECT: "Revenue reached $45.2M" (uses exact provided number)
WRONG: "Revenue grew 12%" (calculated percentage not provided)

CORRECT: "Operating expenses at $35.8M exceeded normal range"  
WRONG: "Due to new acquisitions and market expansion" (external context not provided)

=== YOUR DATA ===
Current Quarter: {json.dumps(data_summary['latest_quarter'], indent=2)}
Statistical Anomalies: {json.dumps(anomalies, indent=2)}

Requirements:
1. Executive summary: 3-5 sentences using ONLY provided numbers
2. Each anomaly: explanation under 200 chars, next steps under 150 chars
3. Risk levels: High/Medium/Low only
4. NO calculations, percentages, or external context unless explicitly provided

Use the financial_analysis tool with exact schema compliance."""

                        response = client.messages.create(
                            model="claude-3-5-haiku-20241022",
                            max_tokens=1500,  # Reduced for cost control
                            temperature=0.1,  # Low temperature for determinism
                            tools=[analysis_tool],
                            messages=[{"role": "user", "content": prompt}]
                        )
                        
                        # Parse response
                        tool_result = None
                        for content in response.content:
                            if content.type == "tool_use":
                                tool_result = content.input
                                break
                        
                        if tool_result:
                            # Add timestamp
                            tool_result["analysis_timestamp"] = datetime.now().isoformat()
                            analysis = FinancialAnalysis(**tool_result)
                    
                    except Exception as e:
                        st.error(f"Error generating analysis: {str(e)}")
                        st.info("Falling back to basic analysis...")
                        
                        # Fallback analysis
                        analysis = FinancialAnalysis(
                            executive_summary=ExecutiveSummary(
                                narrative="Analysis completed successfully. Review the detected anomalies for potential risks.",
                                key_metrics=[col for col in df.columns if df[col].dtype in ['int64', 'float64']][:5],
                                data_sources=df.columns.tolist()[:5]
                            ),
                            anomalies=[
                                Anomaly(
                                    metric=anomaly['metric'],
                                    current_value=f"{anomaly['value']:,.2f}",
                                    comparison_value="N/A",
                                    change_percent="N/A", 
                                    risk_level="Medium" if anomaly['z_score'] < 3 else "High",
                                    explanation=f"Statistical outlier detected with z-score of {anomaly['z_score']:.2f}",
                                    next_steps="Review data source and validate business context",
                                    z_score=anomaly['z_score']
                                ) for anomaly in anomalies[:5]  # Limit to 5 anomalies
                            ],
                            total_anomalies_found=len(anomalies),
                            analysis_timestamp=datetime.now().isoformat()
                        )
                
                if analysis:
                    # Display results
                    st.header("ðŸ“ AI-Generated Analysis")
                    
                    # Executive Summary
                    st.subheader("Executive Summary")
                    st.write(analysis.executive_summary.narrative)
                    
                    # Key Metrics
                    with st.expander("Key Metrics Referenced"):
                        for metric in analysis.executive_summary.key_metrics:
                            st.write(f"â€¢ {metric}")
                    
                    # Anomaly Analysis
                    if analysis.anomalies:
                        st.subheader("ðŸš¨ Risk & Anomaly Analysis")
                        
                        for i, anomaly in enumerate(analysis.anomalies, 1):
                            with st.container():
                                col1, col2 = st.columns([3, 1])
                                
                                with col1:
                                    st.write(f"**{i}. {anomaly.metric}**")
                                    st.write(f"*Current:* {anomaly.current_value}")
                                    if anomaly.comparison_value and anomaly.comparison_value != "N/A":
                                        st.write(f"*Previous:* {anomaly.comparison_value}")
                                    if anomaly.change_percent and anomaly.change_percent != "N/A":
                                        st.write(f"*Change:* {anomaly.change_percent}")
                                    st.write(f"*Analysis:* {anomaly.explanation}")
                                    st.write(f"*Next Steps:* {anomaly.next_steps}")
                                
                                with col2:
                                    risk_color = {"High": "ðŸ”´", "Medium": "ðŸŸ¡", "Low": "ðŸŸ¢"}
                                    st.metric(
                                        "Risk Level", 
                                        f"{risk_color.get(anomaly.risk_level, 'âšª')} {anomaly.risk_level}",
                                        f"z-score: {anomaly.z_score:.2f}"
                                    )
                                
                                st.divider()
                    
                    # Export options
                    st.header("ðŸ“¤ Export Analysis")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        if st.button("ðŸ“„ Download Word Report"):
                            doc_bytes = export_to_word(analysis)
                            st.download_button(
                                label="ðŸ’¾ Download DOCX",
                                data=doc_bytes,
                                file_name="financial_analysis_report.docx",
                                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                            )
                    
                    with col2:
                        json_data = analysis.model_dump_json(indent=2)
                        st.download_button(
                            label="ðŸ“Š Download JSON",
                            data=json_data,
                            file_name="financial_analysis.json",
                            mime="application/json"
                        )
                    
                    with col3:
                        audit_report = generate_audit_trail_report(analysis, df_with_changes, anomalies)
                        st.download_button(
                            label="ðŸ” Download Audit Trail", 
                            data=audit_report,
                            file_name="audit_trail_report.txt",
                            mime="text/plain"
                        )
                    
                    # Audit trail
                    conn.execute('''
                        INSERT INTO analysis_log 
                        (timestamp, input_file_name, anomalies_detected, prompt_used, ai_response) 
                        VALUES (?, ?, ?, ?, ?)
                    ''', (
                        datetime.now().isoformat(),
                        uploaded_file.name,
                        len(anomalies),
                        "Financial analysis prompt",
                        json.dumps(analysis.model_dump(), indent=2)
                    ))
                    conn.commit()
                    
                    st.success("âœ… Analysis complete! Audit trail logged.")
        
        except Exception as e:
            st.error(f"Error processing file: {str(e)}")
    
    # Footer
    st.markdown("---")
    st.markdown("*AI-powered anomaly detection and summarization for enterprise reporting*")

if __name__ == "__main__":
    main()